{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Classifier-TensorFlow-Demo\n",
    "\n",
    "In this demonstration I will train an image classifier using the PyTorch Framework to classify 102 species of flowers. For training I will be using [this dataset](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html) of 102 flower categories. You can see a few examples below.\n",
    "\n",
    "<img src='assets/Flowers.png' width=500px>\n",
    "\n",
    "The project is broken down into multiple steps:\n",
    "\n",
    "* Load and preprocess the image dataset\n",
    "* Train the image classifier on the dataset\n",
    "* Use the trained classifier to predict image content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Use GPU if it's available, else use cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Run autotuner to select kernel with best peroformance\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Here we'll use `torchvision` to load the data ([documentation](http://pytorch.org/docs/0.3.0/torchvision/index.html)). The dataset is split into three parts, training, validation, and testing. You can [download the dataset here](https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz). For the training, we'll want to apply transformations such as random scaling, cropping, and flipping. This will help the network generalize leading to better performance. We'll also need to make sure the input data is resized to 224x224 pixels as required by the pre-trained networks.\n",
    "\n",
    "The validation and testing sets are used to measure the model's performance on data it hasn't seen yet. For this we don't want any scaling or rotation transformations, but we'll need to resize then crop the images to the appropriate size.\n",
    "\n",
    "The pre-trained networks we'll use were trained on the ImageNet dataset where each color channel was normalized separately. For all three sets we'll need to normalize the means and standard deviations of the images to what the network expects. For the means, it's `[0.485, 0.456, 0.406]` and for the standard deviations `[0.229, 0.224, 0.225]`, calculated from the ImageNet images.  These values will shift each color channel to be centered at 0 and range from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'flowers'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for the training, validation, and testing sets\n",
    "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.RandomResizedCrop(224, scale=(0.5, 1.5)),\n",
    "                                       transforms.ColorJitter(brightness=0.03, hue=0.03),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "valid_test_transforms = transforms.Compose([transforms.Resize(224), \n",
    "                                            transforms.CenterCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                                 [0.229, 0.224, 0.225])])\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
    "\n",
    "valid_data = datasets.ImageFolder(data_dir + '/valid', transform=valid_test_transforms)\n",
    "\n",
    "test_data = datasets.ImageFolder(data_dir + '/test', transform=valid_test_transforms)\n",
    "\n",
    "# Using the image datasets and the trainforms, define the dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=6, pin_memory=True)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(valid_data, batch_size=64, shuffle=False, num_workers=6, pin_memory=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False, num_workers=6, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label mapping\n",
    "\n",
    "We'll need to load in a mapping from category label to category name. We can find this in the file `cat_to_name.json`. It's a JSON object which we can read in with the [`json` module](https://docs.python.org/2/library/json.html). This will give us a dictionary mapping the integer encoded categories to the actual names of the flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and training the model and classifier\n",
    "\n",
    "Now that the data is ready, it's time to build and train the classifier. We will be using one of the pretrained models from `torchvision.models` to get the image features. Many pretrained classification models are available and can be found [here](https://pytorch.org/vision/stable/models.html). We will need to:\n",
    "\n",
    "* Load a pretrained network and freeze the models weights\n",
    "* Define a new, untrained feed-forward network as a classifier, using ReLU activations and dropout\n",
    "* Train the classifier layers using backpropagation using the pre-trained network to get the features\n",
    "* Track the loss and accuracy on the validation set to determine the best hyperparameters\n",
    "\n",
    "When training we will only be updating the weights of the new classifier we build while using the weights of the pretrained model to detect image features. We will then try some hyperparameter combinations (learning rate, units in the classifier, epochs, etc) to see if we can increase the model validation accuracy.\n",
    "\n",
    "First I will define some functions and a class to help build the classifier and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dpflo\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained weights\n",
    "weights='IMAGENET1K_V1'\n",
    "\n",
    "# Load model using pretrained weights\n",
    "model = models.maxvit_t(weights=weights)\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n",
    "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            input_size: integer, size of the input layer\n",
    "            output_size: integer, size of the output layer\n",
    "            hidden_layers: list of integers, the sizes of the hidden layers\n",
    "            drop_p: float, dropout probability\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop_p = drop_p\n",
    "        \n",
    "        # Input to a hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        \n",
    "        # Add a variable number of more hidden layers\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        \n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x))\n",
    "            x = F.dropout(x, self.drop_p)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, validloader, criterion, optimizer, epochs=10, scheduler=None):\n",
    "    ''' Trains the model on training data for number of epochs\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        model: model to train\n",
    "        trainloader: training data loader\n",
    "        validloader: validation data loader\n",
    "        criterion: loss function\n",
    "        optimizer: optimizer to update model parameters\n",
    "        epochs: number of epochs to train the model\n",
    "        scheduler: learning rate scheduler\n",
    "    \n",
    "    '''\n",
    "    # Loss and Accuracy variables are global for plotting and comparing\n",
    "    global train_losses\n",
    "    global validation_losses\n",
    "    global validation_accuracy\n",
    "    \n",
    "    train_losses, validation_losses, validation_accuracy = [], [], []\n",
    "         \n",
    "    # Move model to GPU if available, else CPU is used\n",
    "    model.to(device) \n",
    "       \n",
    "    # Loop over epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Start timer\n",
    "        start = time.time() \n",
    "        \n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Keep track of training epoch loss rate\n",
    "        running_loss = 0 \n",
    "\n",
    "        # Get our data\n",
    "        for images,labels in trainloader:\n",
    "\n",
    "            # Move image and label tensors to the default device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Clear the gradients, do this because gradients are accumulated\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass, then backward pass, then update weights\n",
    "            log_ps = model.forward(images) # Forward Pass. Ouputs log probabilities\n",
    "            loss = criterion(log_ps, labels) # Calculate Loss\n",
    "            loss.backward() # Backpropagation\n",
    "            optimizer.step() # Update Weights        \n",
    "            running_loss += loss.item() # Accumulate training loss\n",
    "\n",
    "        else: # After all training batches in current epoch are complete\n",
    "\n",
    "            # Update learning rate if scheduler is supplied\n",
    "            if scheduler:\n",
    "                scheduler.step() \n",
    "\n",
    "            # Turn off gradients to speed up this part\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Set model to evaluation mode/ inference mode. Turns off dropout\n",
    "                model.eval()\n",
    "\n",
    "                # Keep track of validation loss and accuracy\n",
    "                validation_loss = 0\n",
    "                accuracy = 0\n",
    "\n",
    "                # Validation pass here\n",
    "                # Loop over validation data\n",
    "                for images, labels in validloader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                    log_ps = model.forward(images) # Forward Pass. Ouputs log probabilities\n",
    "                    batch_loss = criterion(log_ps, labels) # Calculate Loss\n",
    "                    validation_loss += batch_loss.item() # Accumulate validation loss\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    ps = torch.exp(log_ps) # Exponential of log probabilities for each class\n",
    "                    top_p, top_class = ps.topk(1, dim=1) # Get top 1 predictions\n",
    "                    equals = top_class == labels.view(*top_class.shape) # Compare prediction with ground truth\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)) # Accumulate accuracy scores\n",
    "\n",
    "            # Get mean loss to enable comparison between train and validation sets\n",
    "            train_losses.append(running_loss/ len(trainloader))\n",
    "            validation_losses.append(validation_loss/ len(validloader))\n",
    "\n",
    "            # Get mean validation accuracy\n",
    "            validation_accuracy.append(accuracy/ len(validloader))\n",
    "\n",
    "            if (epoch+1) == 1 or ((epoch+1) % 5) == 0: # Print 1st and every 5 epochs\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, \"\n",
    "                      f\"Time per epoch: {time.time() - start:.2f} seconds, \"\n",
    "                      f\"Training Loss: {train_losses[-1]:.4f}, \"\n",
    "                      f\"Validation Loss: {validation_losses[-1]:.4f}, \"\n",
    "                      f\"Validation accuracy: {validation_accuracy[-1]:.4f}\")\n",
    "                \n",
    "             # Set the model to training mode\n",
    "            model.train()    \n",
    "            \n",
    "    print(f\"Final Epoch {epoch+1}/{epochs}, Validation accuracy: {validation_accuracy[-1]:.4f}\")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Dictionary\n",
    "hyperparameters = {0: {'hidden_units':[1024], 'learning_rate':0.001},\n",
    "                   1: {'hidden_units':[1024], 'learning_rate':0.003},\n",
    "                   2: {'hidden_units':[1024,512], 'learning_rate':0.001},\n",
    "                   3: {'hidden_units':[1024,512], 'learning_rate':0.003},\n",
    "                   4: {'hidden_units':[512], 'learning_rate':0.001},\n",
    "                   5: {'hidden_units':[512], 'learning_rate':0.003}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current Training Hyperparameters: Hidden_layer_Size(s): [1024], Learning_Rate: 0.001\n",
      "Epoch 1/15, Time per epoch: 84.28 seconds, Training Loss: 2.1580, Validation Loss: 0.7415, Validation accuracy: 0.8394\n",
      "Epoch 5/15, Time per epoch: 30.54 seconds, Training Loss: 0.1996, Validation Loss: 0.3235, Validation accuracy: 0.9154\n",
      "Epoch 10/15, Time per epoch: 31.45 seconds, Training Loss: 0.1004, Validation Loss: 0.2228, Validation accuracy: 0.9437\n",
      "Epoch 15/15, Time per epoch: 55.07 seconds, Training Loss: 0.0715, Validation Loss: 0.2332, Validation accuracy: 0.9368\n",
      "Final Epoch 15/15, Validation accuracy: 0.9368\n",
      "Best current hyperparameter validation accuracy: 0.9440\n",
      "\n",
      "Saving Model with Best Validation Accuracy over all hyperparameters: 0.9440\n",
      "Best hyperparameters: {'hidden_units': [1024], 'learning_rate': 0.001}\n",
      "Model saved to: 'model_best_val_accuracy.pth'\n",
      "\n",
      "\n",
      "Current Training Hyperparameters: Hidden_layer_Size(s): [1024], Learning_Rate: 0.003\n",
      "Epoch 1/15, Time per epoch: 55.54 seconds, Training Loss: 1.4724, Validation Loss: 0.5495, Validation accuracy: 0.8442\n",
      "Epoch 5/15, Time per epoch: 56.16 seconds, Training Loss: 0.1723, Validation Loss: 0.3741, Validation accuracy: 0.8963\n",
      "Epoch 10/15, Time per epoch: 33.15 seconds, Training Loss: 0.0586, Validation Loss: 0.2696, Validation accuracy: 0.9365\n",
      "Epoch 15/15, Time per epoch: 60.22 seconds, Training Loss: 0.0358, Validation Loss: 0.2374, Validation accuracy: 0.9449\n",
      "Final Epoch 15/15, Validation accuracy: 0.9449\n",
      "Best current hyperparameter validation accuracy: 0.9521\n",
      "\n",
      "Saving Model with Best Validation Accuracy over all hyperparameters: 0.9521\n",
      "Best hyperparameters: {'hidden_units': [1024], 'learning_rate': 0.003}\n",
      "Model saved to: 'model_best_val_accuracy.pth'\n",
      "\n",
      "\n",
      "Current Training Hyperparameters: Hidden_layer_Size(s): [1024, 512], Learning_Rate: 0.001\n",
      "Epoch 1/15, Time per epoch: 58.94 seconds, Training Loss: 2.3815, Validation Loss: 0.9090, Validation accuracy: 0.7843\n",
      "Epoch 5/15, Time per epoch: 59.67 seconds, Training Loss: 0.2598, Validation Loss: 0.4168, Validation accuracy: 0.8845\n",
      "Epoch 10/15, Time per epoch: 30.18 seconds, Training Loss: 0.1166, Validation Loss: 0.2988, Validation accuracy: 0.9217\n",
      "Epoch 15/15, Time per epoch: 58.59 seconds, Training Loss: 0.0809, Validation Loss: 0.2807, Validation accuracy: 0.9241\n",
      "Final Epoch 15/15, Validation accuracy: 0.9241\n",
      "Best current hyperparameter validation accuracy: 0.9403\n",
      "\n",
      "Current Training Hyperparameters: Hidden_layer_Size(s): [1024, 512], Learning_Rate: 0.003\n",
      "Epoch 1/15, Time per epoch: 51.54 seconds, Training Loss: 1.8876, Validation Loss: 0.7942, Validation accuracy: 0.7691\n",
      "Epoch 5/15, Time per epoch: 30.43 seconds, Training Loss: 0.2561, Validation Loss: 0.4129, Validation accuracy: 0.9017\n",
      "Epoch 10/15, Time per epoch: 29.94 seconds, Training Loss: 0.0945, Validation Loss: 0.3253, Validation accuracy: 0.9229\n",
      "Epoch 15/15, Time per epoch: 30.15 seconds, Training Loss: 0.0465, Validation Loss: 0.3138, Validation accuracy: 0.9301\n",
      "Final Epoch 15/15, Validation accuracy: 0.9301\n",
      "Best current hyperparameter validation accuracy: 0.9461\n",
      "\n",
      "Current Training Hyperparameters: Hidden_layer_Size(s): [512], Learning_Rate: 0.001\n",
      "Epoch 1/15, Time per epoch: 30.60 seconds, Training Loss: 2.5795, Validation Loss: 1.0341, Validation accuracy: 0.7985\n",
      "Epoch 5/15, Time per epoch: 30.48 seconds, Training Loss: 0.2636, Validation Loss: 0.3575, Validation accuracy: 0.9018\n",
      "Epoch 10/15, Time per epoch: 30.35 seconds, Training Loss: 0.1357, Validation Loss: 0.2700, Validation accuracy: 0.9301\n",
      "Epoch 15/15, Time per epoch: 30.16 seconds, Training Loss: 0.1151, Validation Loss: 0.2642, Validation accuracy: 0.9277\n",
      "Final Epoch 15/15, Validation accuracy: 0.9277\n",
      "Best current hyperparameter validation accuracy: 0.9374\n",
      "\n",
      "Current Training Hyperparameters: Hidden_layer_Size(s): [512], Learning_Rate: 0.003\n",
      "Epoch 1/15, Time per epoch: 30.25 seconds, Training Loss: 1.6450, Validation Loss: 0.6717, Validation accuracy: 0.8250\n",
      "Epoch 5/15, Time per epoch: 30.25 seconds, Training Loss: 0.1817, Validation Loss: 0.3185, Validation accuracy: 0.9112\n",
      "Epoch 10/15, Time per epoch: 30.36 seconds, Training Loss: 0.0691, Validation Loss: 0.2572, Validation accuracy: 0.9325\n",
      "Epoch 15/15, Time per epoch: 30.21 seconds, Training Loss: 0.0540, Validation Loss: 0.1990, Validation accuracy: 0.9482\n",
      "Final Epoch 15/15, Validation accuracy: 0.9482\n",
      "Best current hyperparameter validation accuracy: 0.9485\n",
      "\n",
      "Done Training!\n",
      "Best Validation Accuracy over all hyperparameters: 0.9521\n",
      "Best hyperparameters: {'hidden_units': [1024], 'learning_rate': 0.003}\n",
      "Model saved to: 'model_best_val_accuracy.pth'\n"
     ]
    }
   ],
   "source": [
    "# List to hold each hyperparameter combos best validation accuracy\n",
    "best_validation_accuracies = []\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "# Loop through hyperparameter combinations\n",
    "for i in range(len(hyperparameters)):\n",
    "    hidden_layers, learning_rate = hyperparameters[i]['hidden_units'], hyperparameters[i]['learning_rate']\n",
    "    \n",
    "    # Print current hyperparameters\n",
    "    print(f\"\\nCurrent Training Hyperparameters: Hidden_layer_Size(s): {hidden_layers}, Learning_Rate: {learning_rate}\")\n",
    "    \n",
    "    # Define new classifier\n",
    "    new_classifier = Network(512, 102, hidden_layers, drop_p=0.1)\n",
    "    \n",
    "    # Replace pretrained model output classifier layer[5] with newly created classifier\n",
    "    model.classifier[5] = new_classifier\n",
    "    \n",
    "    # Define Loss Functions\n",
    "    criterion = nn.NLLLoss() # Negative Log Likelihood Loss\n",
    "    \n",
    "    # Optimizer only updating new classifier layer[5] weights\n",
    "    optimizer = optim.Adam(model.classifier[5].parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Train model using training function with hyperparameters\n",
    "    train(model, trainloader, validloader, criterion, optimizer, epochs=epochs, scheduler=scheduler)\n",
    "    \n",
    "    # Best validation accuracy for this hyperparameter combination after completing all training epochs\n",
    "    best_validation_accuracies.append(max(validation_accuracy))      \n",
    "    \n",
    "    print(f\"Best current hyperparameter validation accuracy: {best_validation_accuracies[-1]:.4f}\")\n",
    "    \n",
    "    # Save model if validation accuracy is the highest\n",
    "    \n",
    "    model_name = 'model_best_val_accuracy.pth'\n",
    "    \n",
    "    if best_validation_accuracies[-1] == max(best_validation_accuracies):\n",
    "        best_hyperparamters =  hyperparameters[i] # Store hyperparameters for best validation accuracy\n",
    "        print(f\"\\nSaving Model with Best Validation Accuracy over all hyperparameters: {best_validation_accuracies[-1]:.4f}\"\n",
    "              f\"\\nBest hyperparameters: {best_hyperparamters}\"\n",
    "              f\"\\nModel saved to: '{model_name}'\\n\")\n",
    "        \n",
    "        torch.save({'state_dict': model.state_dict(),\n",
    "                    'classifier': model.classifier[5],\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'class_to_idx': train_data.class_to_idx\n",
    "                    }, model_name)\n",
    "        \n",
    "print(f\"\\nDone Training!\"\n",
    "      f\"\\nBest Validation Accuracy over all hyperparameters: {max(best_validation_accuracies):.4f}\"\n",
    "      f\"\\nBest hyperparameters: {best_hyperparamters}\"\n",
    "      f\"\\nModel saved to: '{model_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your network\n",
    "\n",
    "It's good practice to test the trained network on test data, images the network has never seen either in training or validation. This will give us a good estimate for the model's performance on completely new images. We will run the test images through the network and measure the accuracy, the same way we did with validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
